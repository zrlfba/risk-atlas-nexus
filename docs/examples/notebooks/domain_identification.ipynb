{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### This notebook illustrates how to identify AI tasks based on specific use cases.\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "##### Import libraries\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "/Users/dhaval/Projects/Usage-Governance/risk-atlas-nexus/src/risk_atlas_nexus/toolkit/job_utils.py:2: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                  "  from tqdm.autonotebook import tqdm\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "RAN77876513E\n"
               ]
            }
         ],
         "source": [
            "from risk_atlas_nexus.blocks.inference import (\n",
            "    RITSInferenceEngine,\n",
            "    WMLInferenceEngine,\n",
            "    OllamaInferenceEngine,\n",
            "    VLLMInferenceEngine,\n",
            ")\n",
            "from risk_atlas_nexus.blocks.inference.params import (\n",
            "    InferenceEngineCredentials,\n",
            "    RITSInferenceEngineParams,\n",
            "    WMLInferenceEngineParams,\n",
            "    OllamaInferenceEngineParams,\n",
            "    VLLMInferenceEngineParams,\n",
            ")\n",
            "from risk_atlas_nexus.library import RiskAtlasNexus"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "##### Risk Atlas Nexus uses Large Language Models (LLMs) to infer risks dimensions. Therefore requires access to LLMs to inference or call the model.\n",
            "\n",
            "**Available Inference Engines**: WML, Ollama, vLLM, RITS. Please follow the [Inference APIs](https://github.com/IBM/risk-atlas-nexus?tab=readme-ov-file#install-for-inference-apis) guide before going ahead.\n",
            "\n",
            "_Note:_ RITS is intended solely for internal IBM use and requires TUNNELALL VPN for access.\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[2025-05-25 22:40:11:817] - INFO - RiskAtlasNexus - OLLAMA inference engine will execute requests on the server at http://localhost:11434.\n",
                  "[2025-05-25 22:40:11:969] - INFO - RiskAtlasNexus - Created OLLAMA inference engine.\n"
               ]
            }
         ],
         "source": [
            "inference_engine = OllamaInferenceEngine(\n",
            "    model_name_or_path=\"granite3.2:8b\",\n",
            "    credentials=InferenceEngineCredentials(api_url=\"OLLAMA_API_URL\"),\n",
            "    parameters=OllamaInferenceEngineParams(\n",
            "        num_predict=1000, temperature=0, repeat_penalty=1, num_ctx=8192\n",
            "    ),\n",
            ")\n",
            "\n",
            "# inference_engine = WMLInferenceEngine(\n",
            "#     model_name_or_path=\"ibm/granite-20b-code-instruct\",\n",
            "#     credentials={\n",
            "#         \"api_key\": \"WML_API_KEY\",\n",
            "#         \"api_url\": \"WML_API_URL\",\n",
            "#         \"project_id\": \"WML_PROJECT_ID\",\n",
            "#     },\n",
            "#     parameters=WMLInferenceEngineParams(\n",
            "#         max_new_tokens=1000, decoding_method=\"greedy\", repetition_penalty=1\n",
            "#     ),\n",
            "# )\n",
            "\n",
            "# inference_engine = VLLMInferenceEngine(\n",
            "#     model_name_or_path=\"ibm-granite/granite-3.1-8b-instruct\",\n",
            "#     credentials=InferenceEngineCredentials(\n",
            "#         api_url=\"VLLM_API_URL\", api_key=\"VLLM_API_KEY\"\n",
            "#     ),\n",
            "#     parameters=VLLMInferenceEngineParams(max_tokens=1000, temperature=0.7),\n",
            "# )\n",
            "\n",
            "# inference_engine = RITSInferenceEngine(\n",
            "#     model_name_or_path=\"ibm/granite-20b-code-instruct\",\n",
            "#     credentials={\n",
            "#         \"api_key\": \"RITS_API_KEY\",\n",
            "#         \"api_url\": \"RITS_API_URL\",\n",
            "#     },\n",
            "#     parameters=RITSInferenceEngineParams(max_tokens=1000, temperature=0.7),\n",
            "# )"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "##### Create an instance of RiskAtlasNexus\n",
            "\n",
            "_Note: (Optional)_ You can specify your own directory in `RiskAtlasNexus(base_dir=<PATH>)` to utilize custom AI ontologies. If left blank, the system will use the provided AI ontologies.\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[2025-05-25 22:40:12:40] - INFO - RiskAtlasNexus - Created RiskAtlasNexus instance. Base_dir: None\n"
               ]
            }
         ],
         "source": [
            "risk_atlas_nexus = RiskAtlasNexus()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "##### AI Domain Identification API\n",
            "\n",
            "RiskAtlasNexus.identify_domain_from_usecases()\n",
            "\n",
            "Params:\n",
            "\n",
            "- usecases (List[str]): A List of strings describing AI usecases\n",
            "- inference_engine (InferenceEngine): An LLM inference engine to identify AI tasks from usecases.\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Inferring with OLLAMA: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "{'answer': 'Customer service/support',\n",
                     " 'explanation': 'Since the task is related to generating responses and recommendations for customer support agents, this falls under the Customer service/support domain. It involves creating personalized interactions with customers to enhance their experience and support agent efficiency.'}"
                  ]
               },
               "execution_count": 5,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "usecase = \"Generate personalized, relevant responses, recommendations, and summaries of claims for customers to support agents to enhance their interactions with customers.\"\n",
            "\n",
            "risks = risk_atlas_nexus.identify_domain_from_usecases(\n",
            "    usecases=[usecase],\n",
            "    inference_engine=inference_engine,\n",
            ")\n",
            "\n",
            "risks[0].prediction"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "nexus",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.11.4"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
